# awesome-optimizers

A curated list of optimizers for machine learning.

| Symbol | Meaning |
|-|-|
| ‚úîÔ∏è | Native support from PyTorch / TensorFlow |
| üü° | Non-native implementation |
| ‚ùå | No implementation found |

**TIP**: Click on ‚úîÔ∏è or üü° to view the implementation of that optimizer!

| Name | Date | Paper | PyTorch | TensorFlow |
|:----:|:----:|:------|:-------:|:----------:|
| SGD | | | [‚úîÔ∏è](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) | [‚úîÔ∏è](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) |
| RPROP | 1992 | [RPROP - A Fast Adaptive Learning Algorithm](https://www.semanticscholar.org/paper/RPROP-A-Fast-Adaptive-Learning-Algorithm-Riedmiller-Braun/02597de11d6b808ed0a4019f411f9ac7a9d426cb) | [‚úîÔ∏è](https://pytorch.org/docs/stable/optim.html#torch.optim.Rprop) | ‚ùå |
| ASGD | 1992 | [Acceleration of stochastic approximation by averaging](https://dl.acm.org/doi/10.1137/0330046) | [‚úîÔ∏è](https://pytorch.org/docs/stable/optim.html#torch.optim.ASGD) | ‚ùå |
| Adagrad | 2011 | [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://jmlr.csail.mit.edu/papers/v12/duchi11a.html) | [‚úîÔ∏è](https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad) | [‚úîÔ∏è](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad) |
| Adadelta | 2012 | [ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701) | [‚úîÔ∏è](https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta) | [‚úîÔ∏è](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta) |
| RMSprop | 2012 | [Neural Networks for Machine Learning Lecture 6a: Overview of mini-batch gradient descent](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) | [‚úîÔ∏è](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop) | [‚úîÔ∏è](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop) |
| Adam | 2014 | [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) | [‚úîÔ∏è](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) | [‚úîÔ∏è](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) |
| Adamax | 2014 | [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) | [‚úîÔ∏è](https://pytorch.org/docs/stable/optim.html#torch.optim.Adamax) | [‚úîÔ∏è](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax) |
| Nadam | 2015 | [Incorporating Nesterov Momentum into Adam](http://cs229.stanford.edu/proj2015/054_report.pdf) | ‚ùå | [‚úîÔ∏è](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Nadam) |
| AdamW | 2017 | [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) | [‚úîÔ∏è](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW) | [üü°](https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/AdamW) |
| Yogi | 2018 | [Adaptive Methods for Nonconvex Optimization](https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization) | [üü°](https://github.com/jettify/pytorch-optimizer#yogi) | [üü°](https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/Yogi) |
| AMSGrad | 2018 | [On the Convergence of Adam and Beyond](https://arxiv.org/abs/1904.09237) | [‚úîÔ∏è](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) | ‚ùå |
| AdaBound | 2019 | [Adaptive Gradient Methods with Dynamic Bound of Learning Rate](https://arxiv.org/abs/1902.09843) | [üü°](https://github.com/Luolc/AdaBound) | ‚ùå |
| AMSBound | 2019 | [Adaptive Gradient Methods with Dynamic Bound of Learning Rate](https://arxiv.org/abs/1902.09843) | [üü°](https://github.com/Luolc/AdaBound) | ‚ùå |
| LAMB | 2019 | [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962) |  ‚ùå | [üü°](https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/LAMB) |
| RAdam | 2019 | [On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265) | [üü°](https://github.com/LiyuanLucasLiu/RAdam) | [üü°](https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/RectifiedAdam) |
| SGDP | 2020 | [Slowing Down the Weight Norm Increase in Momentum-based Optimizers](https://arxiv.org/abs/2006.08217) | [üü°](https://github.com/clovaai/adamp) | ‚ùå |
| AdamP | 2020 | [Slowing Down the Weight Norm Increase in Momentum-based Optimizers](https://arxiv.org/abs/2006.08217) | [üü°](https://github.com/clovaai/adamp) | ‚ùå |
| Madam | 2020 | [Learning compositional functions via multiplicative weight updates](https://arxiv.org/abs/2006.14560) | [üü°](https://github.com/jxbz/madam) | ‚ùå |
| SuperAdam | 2021 | [Learning compositional functions via multiplicative weight updates](https://par.nsf.gov/servlets/purl/10316115) | [‚úîÔ∏è](https://github.com/LIJUNYI95/SuperAdam) | ‚ùå |


## Other Resources

- [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)
- [torch-optimizer -- collection of optimizers for PyTorch compatible with optim module.](https://github.com/jettify/pytorch-optimizer)
